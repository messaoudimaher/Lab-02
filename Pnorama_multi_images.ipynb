{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dc11g_tMHYPW"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/content/drive/MyDrive/Lab 02'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install imutils\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2p7lPzYDHiNg",
        "outputId": "cf70a1aa-6726-40f9-9da7-535159a87f95"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: imutils in /usr/local/lib/python3.10/dist-packages (0.5.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install opencv-contrib-python\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "007K6paYHnVz",
        "outputId": "931e02ef-fcca-4f0c-ce3d-2a1f1f6680c5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.10/dist-packages (4.8.0.76)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-contrib-python) (1.25.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from imutils import paths\n",
        "import imutils\n",
        "import cv2\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "cgGffeppHwT3"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#class\n",
        "class utils:\n",
        "    def loadImages(path,resize):\n",
        "        '''Load Images from path to array, @param path is the folder which containing images, @param resize is True\n",
        "        if image is halved in size, otherwise is False'''\n",
        "        image_path = list(paths.list_images(path))\n",
        "        list_image = []\n",
        "        for i,j in enumerate(image_path):\n",
        "            image = cv2.imread(j)\n",
        "            if resize==1:\n",
        "                image=cv2.resize(image,(int(image.shape[1]/4),int(image.shape[0]/4)))\n",
        "            list_image.append(image)\n",
        "        return (list_image)\n",
        "\n",
        "    def trim(frame):\n",
        "        '''crop frame '''\n",
        "        #crop top\n",
        "        if not np.sum(frame[0]):\n",
        "            return trim(frame[1:])\n",
        "        #crop bottom\n",
        "        elif not np.sum(frame[-1]):\n",
        "            return trim(frame[:-2])\n",
        "        #crop left\n",
        "        elif not np.sum(frame[:,0]):\n",
        "            return trim(frame[:,1:])\n",
        "        #crop right\n",
        "        elif not np.sum(frame[:,-1]):\n",
        "            return trim(frame[:,:-2])\n",
        "        return frame\n",
        "\n",
        "    def padding(img,top,bottom,left,right):\n",
        "        '''add padding to img'''\n",
        "        border = cv2.copyMakeBorder(\n",
        "            img,\n",
        "            top=top,\n",
        "            bottom=bottom,\n",
        "            left=left,\n",
        "            right=right,\n",
        "            borderType=cv2.BORDER_CONSTANT\n",
        "        )\n",
        "        return border"
      ],
      "metadata": {
        "id": "p_1qxZZHH24g"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class features:\n",
        "    def findAndDescribeFeatures(image,opt='ORB'):\n",
        "        '''find and describe features of @image,\n",
        "        if opt='SURF', SURF algorithm is used.\n",
        "        if opt='SIFT', SIFT algorithm is used.\n",
        "        if opt='ORB', ORB algorithm is used.\n",
        "        @Return keypoints and features of img'''\n",
        "        #Getting gray image\n",
        "        grayImage = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "        if opt=='SURF':\n",
        "            md = cv2.xfeatures2d.SURF_create()\n",
        "        if opt=='ORB':\n",
        "            md = cv2.ORB_create(nfeatures=3000)\n",
        "        if opt=='SIFT':\n",
        "            md = cv2.xfeatures2d.SIFT_create()\n",
        "        #Find interest points and Computing features.\n",
        "        keypoints, features = md.detectAndCompute(grayImage, None)\n",
        "        #Converting keypoints to numbers.\n",
        "        #keypoints = np.float32(keypoints)\n",
        "        features = np.float32(features)\n",
        "        return keypoints, features\n",
        "\n",
        "    def matchFeatures(featuresA, featuresB,ratio=0.75,opt='FB'):\n",
        "        '''matching features beetween 2 @features.\n",
        "         If opt='FB', FlannBased algorithm is used.\n",
        "         If opt='BF', BruteForce algorithm is used.\n",
        "         @ratio is the Lowe's ratio test.\n",
        "         @return matches'''\n",
        "        if opt=='BF':\n",
        "            featureMatcher = cv2.DescriptorMatcher_create(\"BruteForce\")\n",
        "        if opt=='FB':\n",
        "            #featureMatcher = cv2.DescriptorMatcher_create(\"FlannBased\")\n",
        "            FLANN_INDEX_KDTREE = 0\n",
        "            index_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 5)\n",
        "            search_params = dict(checks = 50)\n",
        "            featureMatcher = cv2.FlannBasedMatcher(index_params, search_params)\n",
        "\n",
        "\n",
        "        #performs k-NN matching between the two feature vector sets using k=2\n",
        "        #(indicating the top two matches for each feature vector are returned).\n",
        "        matches = featureMatcher.knnMatch(featuresA,featuresB, k=2)\n",
        "\n",
        "        # store all the good matches as per Lowe's ratio test.\n",
        "        good = []\n",
        "        for m,n in matches:\n",
        "            if m.distance<ratio*n.distance:\n",
        "                good.append(m)\n",
        "        if len(good)>4:\n",
        "            return good\n",
        "        else:\n",
        "            raise Exception(\"Not enought matches\")\n",
        "\n",
        "    def generateHomography(src_img, dst_img, ransacRep=5.0):\n",
        "        '''@Return Homography matrix, @param src_img is the image which is warped by homography,\n",
        "        @param dst_img is the image which is choosing as pivot, @param ratio is the David Lowe’s ratio,\n",
        "        @param ransacRep is the maximum pixel “wiggle room” allowed by the RANSAC algorithm\n",
        "        '''\n",
        "\n",
        "        src_kp,src_features=features.findAndDescribeFeatures(src_img)\n",
        "        dst_kp,dst_features=features.findAndDescribeFeatures(dst_img)\n",
        "\n",
        "        good=features.matchFeatures(src_features,dst_features)\n",
        "\n",
        "        src_points = np.float32([src_kp[m.queryIdx].pt for m in good]).reshape(-1,1,2)\n",
        "        dst_points = np.float32([dst_kp[m.trainIdx].pt for m in good]).reshape(-1,1,2)\n",
        "\n",
        "        H, mask  = cv2.findHomography(src_points, dst_points, cv2.RANSAC,ransacRep)\n",
        "        matchesMask = mask.ravel().tolist()\n",
        "        return H,matchesMask\n",
        "\n",
        "\n",
        "    def drawKeypoints(img,kp):\n",
        "        img1=img\n",
        "        cv2.drawKeypoints(img,kp,img1,flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
        "        return img1\n",
        "\n",
        "    def drawMatches(src_img,src_kp,dst_img,dst_kp,matches,matchesMask):\n",
        "        draw_params = dict(matchColor = (0,255,0), # draw matches in green color\n",
        "                       singlePointColor = None,\n",
        "                       matchesMask = matchesMask[:100], # draw only inliers\n",
        "                       flags = 2)\n",
        "        return cv2.drawMatches(src_img,src_kp,dst_img,dst_kp,matches[:],None,**draw_params)"
      ],
      "metadata": {
        "id": "TBGoJ_R9H7jc"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class stitch:\n",
        "    def blendingMask(height, width, barrier, smoothing_window, left_biased=True):\n",
        "        assert barrier < width\n",
        "        mask = np.zeros((height, width))\n",
        "\n",
        "        offset = int(smoothing_window/2)\n",
        "        try:\n",
        "            if left_biased:\n",
        "                mask[:,barrier-offset:barrier+offset+1]=np.tile(np.linspace(1,0,2*offset+1).T, (height, 1))\n",
        "                mask[:,:barrier-offset] = 1\n",
        "            else:\n",
        "                mask[:,barrier-offset:barrier+offset+1]=np.tile(np.linspace(0,1,2*offset+1).T, (height, 1))\n",
        "                mask[:,barrier+offset:] = 1\n",
        "        except:\n",
        "            if left_biased:\n",
        "                mask[:,barrier-offset:barrier+offset+1]=np.tile(np.linspace(1,0,2*offset).T, (height, 1))\n",
        "                mask[:,:barrier-offset] = 1\n",
        "            else:\n",
        "                mask[:,barrier-offset:barrier+offset+1]=np.tile(np.linspace(0,1,2*offset).T, (height, 1))\n",
        "                mask[:,barrier+offset:] = 1\n",
        "\n",
        "        return cv2.merge([mask, mask, mask])\n",
        "\n",
        "    def panoramaBlending(dst_img_rz,src_img_warped,width_dst,side,showstep=False):\n",
        "        '''Given two aligned images @dst_img and @src_img_warped, and the @width_dst is width of dst_img\n",
        "        before resize, that indicates where there is the discontinuity between the images,\n",
        "        this function produce a smoothed transient in the overlapping.\n",
        "        @smoothing_window is a parameter that determines the width of the transient\n",
        "        left_biased is a flag that determines whether it is masked the left image,\n",
        "        or the right one'''\n",
        "\n",
        "        h,w,_=dst_img_rz.shape\n",
        "        smoothing_window=int(width_dst/8)\n",
        "        barrier = width_dst -int(smoothing_window/2)\n",
        "        mask1 = stitch.blendingMask(h, w, barrier, smoothing_window = smoothing_window, left_biased = True)\n",
        "        mask2 = stitch.blendingMask(h, w, barrier, smoothing_window = smoothing_window, left_biased = False)\n",
        "\n",
        "        if showstep:\n",
        "            nonblend=src_img_warped+dst_img_rz\n",
        "        else:\n",
        "            nonblend=None\n",
        "            leftside=None\n",
        "            rightside=None\n",
        "\n",
        "        if side=='left':\n",
        "            dst_img_rz=cv2.flip(dst_img_rz,1)\n",
        "            src_img_warped=cv2.flip(src_img_warped,1)\n",
        "            dst_img_rz=(dst_img_rz*mask1)\n",
        "            src_img_warped=(src_img_warped*mask2)\n",
        "            pano=src_img_warped+dst_img_rz\n",
        "            pano=cv2.flip(pano,1)\n",
        "            if showstep:\n",
        "                leftside=cv2.flip(src_img_warped,1)\n",
        "                rightside=cv2.flip(dst_img_rz,1)\n",
        "        else:\n",
        "            dst_img_rz=(dst_img_rz*mask1)\n",
        "            src_img_warped=(src_img_warped*mask2)\n",
        "            pano=src_img_warped+dst_img_rz\n",
        "            if showstep:\n",
        "                leftside=dst_img_rz\n",
        "                rightside=src_img_warped\n",
        "\n",
        "\n",
        "        return pano,nonblend,leftside,rightside\n",
        "\n",
        "    def warpTwoImages(src_img, dst_img,showstep=False):\n",
        "\n",
        "        #generate Homography matrix\n",
        "        H,_=features.generateHomography(src_img,dst_img)\n",
        "\n",
        "        #get height and width of two images\n",
        "        height_src,width_src = src_img.shape[:2]\n",
        "        height_dst,width_dst = dst_img.shape[:2]\n",
        "\n",
        "        #extract conners of two images: top-left, bottom-left, bottom-right, top-right\n",
        "        pts1 = np.float32([[0,0],[0,height_src],[width_src,height_src],[width_src,0]]).reshape(-1,1,2)\n",
        "        pts2 = np.float32([[0,0],[0,height_dst],[width_dst,height_dst],[width_dst,0]]).reshape(-1,1,2)\n",
        "\n",
        "        try:\n",
        "            #aply homography to conners of src_img\n",
        "            pts1_ = cv2.perspectiveTransform(pts1, H)\n",
        "            pts = np.concatenate((pts1_, pts2), axis=0)\n",
        "\n",
        "            #find max min of x,y coordinate\n",
        "            [xmin, ymin] = np.int64(pts.min(axis=0).ravel() - 0.5)\n",
        "            [xmax, ymax] = np.int64(pts.max(axis=0).ravel() + 0.5)\n",
        "            t = [-xmin,-ymin]\n",
        "\n",
        "\n",
        "            #top left point of image which apply homography matrix, which has x coordinate < 0, has side=left\n",
        "            #otherwise side=right\n",
        "            #source image is merged to the left side or right side of destination image\n",
        "            if(pts[0][0][0]<0):\n",
        "                side='left'\n",
        "                width_pano=width_dst+t[0]\n",
        "            else:\n",
        "                width_pano=int(pts1_[3][0][0])\n",
        "                side='right'\n",
        "            height_pano=ymax-ymin\n",
        "\n",
        "            #Translation\n",
        "            Ht = np.array([[1,0,t[0]],[0,1,t[1]],[0,0,1]])\n",
        "            src_img_warped = cv2.warpPerspective(src_img, Ht.dot(H), (width_pano,height_pano))\n",
        "            #generating size of dst_img_rz which has the same size as src_img_warped\n",
        "            dst_img_rz=np.zeros((height_pano,width_pano,3))\n",
        "            if side=='left':\n",
        "                dst_img_rz[t[1]:height_src+t[1],t[0]:width_dst+t[0]] = dst_img\n",
        "            else:\n",
        "                dst_img_rz[t[1]:height_src+t[1],:width_dst] = dst_img\n",
        "\n",
        "            #blending panorama\n",
        "            pano,nonblend,leftside,rightside=stitch.panoramaBlending(dst_img_rz,src_img_warped,width_dst,side,showstep=showstep)\n",
        "\n",
        "            #croping black region\n",
        "            pano=stitch.crop(pano,height_dst,pts)\n",
        "            return pano,nonblend,leftside,rightside\n",
        "        except:\n",
        "            raise Exception(\"Please try again with another image set!\")\n",
        "\n",
        "    def multiStitching(list_images):\n",
        "        '''assume that the list_images was supplied in left-to-right order, choose middle image then\n",
        "        divide the array into 2 sub-arrays, left-array and right-array. Stiching middle image with each\n",
        "        image in 2 sub-arrays. @param list_images is The list which containing images, @param smoothing_window is\n",
        "        the value of smoothy side after stitched, @param output is the folder which containing stitched image\n",
        "        '''\n",
        "        n=int(len(list_images)/2+0.5)\n",
        "        left=list_images[:n]\n",
        "        right=list_images[n-1:]\n",
        "        right.reverse()\n",
        "        while len(left)>1:\n",
        "            dst_img=left.pop()\n",
        "            src_img=left.pop()\n",
        "            left_pano,_,_,_=stitch.warpTwoImages(src_img,dst_img)\n",
        "            left_pano=left_pano.astype('uint8')\n",
        "            left.append(left_pano)\n",
        "\n",
        "        while len(right)>1:\n",
        "            dst_img=right.pop()\n",
        "            src_img=right.pop()\n",
        "            right_pano,_,_,_=stitch.warpTwoImages(src_img,dst_img)\n",
        "            right_pano=right_pano.astype('uint8')\n",
        "            right.append(right_pano)\n",
        "\n",
        "        #if width_right_pano > width_left_pano, Select right_pano as destination. Otherwise is left_pano\n",
        "        if(right_pano.shape[1]>=left_pano.shape[1]):\n",
        "            fullpano,_,_,_=stitch.warpTwoImages(left_pano,right_pano)\n",
        "        else:\n",
        "            fullpano,_,_,_=stitch.warpTwoImages(right_pano,left_pano)\n",
        "        return fullpano\n",
        "\n",
        "    def crop(panorama,h_dst,conners):\n",
        "        '''crop panorama based on destination.\n",
        "        @param panorama is the panorama\n",
        "        @param h_dst is the hight of destination image\n",
        "        @param conner is the tuple which containing 4 conners of warped image and\n",
        "        4 conners of destination image'''\n",
        "        #find max min of x,y coordinate\n",
        "        [xmin, ymin] = np.int32(conners.min(axis=0).ravel() - 0.5)\n",
        "        [xmax, ymax] = np.int32(conners.max(axis=0).ravel() + 0.5)\n",
        "        t = [-xmin,-ymin]\n",
        "        conners=conners.astype(int)\n",
        "\n",
        "        #conners[0][0][0] is the X coordinate of top-left point of warped image\n",
        "        #If it has value<0, warp image is merged to the left side of destination image\n",
        "        #otherwise is merged to the right side of destination image\n",
        "        if conners[0][0][0]<0:\n",
        "            n=abs(-conners[1][0][0]+conners[0][0][0])\n",
        "            panorama=panorama[t[1]:h_dst+t[1],n:,:]\n",
        "        else:\n",
        "            if(conners[2][0][0]<conners[3][0][0]):\n",
        "                panorama=panorama[t[1]:h_dst+t[1],0:conners[2][0][0],:]\n",
        "            else:\n",
        "                panorama=panorama[t[1]:h_dst+t[1],0:conners[3][0][0],:]\n",
        "        return panorama"
      ],
      "metadata": {
        "id": "HYGvj3L6IDQb"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg"
      ],
      "metadata": {
        "id": "-i88NUlYIHiV"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convertResult(img):\n",
        "    '''Because of our images which were loaded by opencv,\n",
        "    in order to display the correct output with matplotlib,\n",
        "    you need to reduce the range of your floating point image from [0,255] to [0,1]\n",
        "    and converting the image from BGR to RGB:'''\n",
        "    img = np.array(img,dtype=float)/float(255)\n",
        "    img = img[:,:,::-1]\n",
        "    return img"
      ],
      "metadata": {
        "id": "6fFbUzW2IKc3"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "List = os.listdir('/content/drive/MyDrive/Lab 02/Pano1x')\n",
        "for i in range(len(List)):\n",
        "  print(List[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bgdf4VkPIPxr",
        "outputId": "fd344674-ec87-4b10-ed26-4e602573d053"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.jpg\n",
            "1.jpg\n",
            "3.jpg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "# Update this path to the path of your folder\n",
        "image_directory = Path(\"/content/drive/MyDrive/Lab 02/Pano1x\")\n",
        "\n",
        "# List all image files in the directory (assuming JPG for the example)\n",
        "#image_files = list(image_directory.glob('*.jpg'))  # Adjust the glob pattern as needed4\n",
        "image_files = [file for file in image_directory.glob('*.jpg')]\n",
        "\n",
        "# Read each image file and add to a list\n",
        "list_images = [mpimg.imread(str(image_file)) for image_file in image_files]"
      ],
      "metadata": {
        "id": "fk8-WhgYIS4_"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#image_1 = mpimg.imread(\"/content/drive/MyDrive/Lab 02/pano/1.jpg\")\n",
        "#image_2 = mpimg.imread(\"/content/drive/MyDrive/Lab 02/pano/2.jpg\")\n",
        "#image_3 = mpimg.imread(\"/content/drive/MyDrive/Lab 02/pano/3.jpg\")\n",
        "#image_4 = mpimg.imread(\"/content/drive/MyDrive/Lab 02/pano/4.jpg\")"
      ],
      "metadata": {
        "id": "7jBbUPaUIjcm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#load images\n",
        "#list_images= [image_1,image_2,image_3,image_4]"
      ],
      "metadata": {
        "id": "TC71iV_SJD82"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#extract keypoints and descriptors using sift\n",
        "k0,f0=features.findAndDescribeFeatures(list_images[0],opt='SIFT')\n",
        "k1,f1=features.findAndDescribeFeatures(list_images[2],opt='SIFT')"
      ],
      "metadata": {
        "id": "_Gmt5-0LJKVz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#draw keypoints\n",
        "img0_kp=features.drawKeypoints(list_images[0],k0)\n",
        "img1_kp=features.drawKeypoints(list_images[1],k1)\n",
        "\n",
        "plt_img = np.concatenate((img0_kp, img1_kp), axis=1)\n",
        "plt.figure(figsize=(15,15))\n",
        "plt.imshow(plt_img)"
      ],
      "metadata": {
        "id": "9KNzxS2EJNV9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#matching features using BruteForce\n",
        "mat=features.matchFeatures(f0,f1,ratio=0.85,opt='BF')"
      ],
      "metadata": {
        "id": "cFvpeAbCJUYL"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Computing Homography matrix and mask\n",
        "H,matMask=features.generateHomography(list_images[0],list_images[1])"
      ],
      "metadata": {
        "id": "3yKOxPMyJnLJ"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#choose list_images[0] as desination\n",
        "pano,non_blend,left_side,right_side=stitch.warpTwoImages(list_images[0],list_images[1],True)"
      ],
      "metadata": {
        "id": "tmV_t9OMJqJU"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#display the leftside of pano before cropping\n",
        "plt.figure(figsize=(15,15))\n",
        "plt.imshow(convertResult(left_side))"
      ],
      "metadata": {
        "id": "jd90ix6FJsnp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#display the rightside of pano before cropping\n",
        "plt.figure(figsize=(15,15))\n",
        "plt.imshow(convertResult(right_side))"
      ],
      "metadata": {
        "id": "nXc3qDS8JvPj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#display pano without cropping and blending\n",
        "plt.figure(figsize=(15,15))\n",
        "plt.imshow(convertResult(non_blend))"
      ],
      "metadata": {
        "id": "y5zZj8_RJxjQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_,non_blend2,_,_=stitch.warpTwoImages(list_images[0],list_images[1],True)\n",
        "plt.figure(figsize=(15,15))\n",
        "plt.imshow(convertResult(non_blend2))"
      ],
      "metadata": {
        "id": "Z2sMEVMUJzuN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#pano after cropping and blending\n",
        "plt.figure(figsize=(15,15))\n",
        "plt.imshow(convertResult(pano))"
      ],
      "metadata": {
        "id": "bKJCysR0J3TZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#multi stitching\n",
        "panorama=stitch.multiStitching(list_images)\n",
        "plt.figure(figsize=(25,15))\n",
        "plt.imshow(convertResult(panorama))"
      ],
      "metadata": {
        "id": "fC639lDfJ6Mm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GFTtpVUpJ8qc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}